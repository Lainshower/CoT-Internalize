{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaoara/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from configuration import MistralConfig\n",
    "from mistral_direct import Mistral\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTAnalysisDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path: str, max_length: int, num_samples: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.eos_token = tokenizer.eos_token\n",
    "        self.examples = self.load_and_process_file(file_path, num_samples)\n",
    "\n",
    "    def load_and_process_file(self, file_path, num_samples):\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            lines = [line.strip().split('||') for line in f.readlines() if len(line.strip().split('||')) == 2]\n",
    "        \n",
    "        sampled_lines = random.sample(lines, min(num_samples, len(lines)))\n",
    "        examples = []\n",
    "        for src, tgt in sampled_lines:\n",
    "            ans = self.extract_answer_w_prefix(tgt)\n",
    "            cot = self.extract_cot_w_prefix(tgt)\n",
    "            example = {\n",
    "                'src': src,\n",
    "                'cot': cot,\n",
    "                'ans': ans\n",
    "            }\n",
    "            examples.append(example)\n",
    "        return examples\n",
    "\n",
    "    def extract_answer_w_prefix(self, text, prefix='####'):\n",
    "        parts = text.split('####', 1)\n",
    "        return prefix + \" \" + parts[1].strip().replace(',', '') if len(parts) > 1 else text\n",
    "\n",
    "    def extract_cot_w_prefix(self, text, prefix=\"\"):\n",
    "        parts = text.split('####', 1)\n",
    "        return prefix + parts[0].strip() if len(parts) > 1 else \"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = self.examples[i]\n",
    "        return example\n",
    "\n",
    "# def split_cot_sentences(cot):\n",
    "#     # '.' 앞뒤가 숫자가 아닌 경우에만 split\n",
    "#     sentences = re.split(r'(?<!\\d)\\.(?!\\d)', cot)\n",
    "#     # 문장 정리 및 필요 시 마침표 추가\n",
    "#     result = []\n",
    "#     for s in sentences:\n",
    "#         s = s.strip()\n",
    "#         if s:\n",
    "#             if s[-1] not in '.!?':\n",
    "#                 s += '.'\n",
    "#             result.append(s)\n",
    "#     return result\n",
    "\n",
    "def split_cot_sentences(cot):\n",
    "    # '.' 앞이 숫자여도 되지만, 뒤에는 숫자가 오면 안 됨\n",
    "    sentences = re.split(r'\\.(?!\\d)', cot)\n",
    "    # 문장 정리 및 필요 시 마침표 추가\n",
    "    result = []\n",
    "    for s in sentences:\n",
    "        s = s.strip()\n",
    "        if s:\n",
    "            if s[-1] not in '.!?':\n",
    "                s += '.'\n",
    "            result.append(s)\n",
    "    return result\n",
    "\n",
    "def format_input(src, cot, ans, eos_token):\n",
    "    return f\"{src} {eos_token} {cot} {eos_token} {ans} {eos_token}\"\n",
    "\n",
    "def get_sep_position(input_ids, sep_id, skip=0):\n",
    "    batch_size = input_ids.shape[0]\n",
    "    sep_positions = input_ids.new_zeros(batch_size).long()\n",
    "    for batch_id in range(batch_size):\n",
    "        mask = input_ids[batch_id].eq(sep_id)\n",
    "        sep_position = mask.nonzero()[0, -1].item()\n",
    "        for _ in range(skip):\n",
    "            mask[sep_position] = False\n",
    "            sep_position = mask.nonzero()[0, -1].item()\n",
    "        sep_positions[batch_id] = sep_position\n",
    "    return sep_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_answer_loss(model, tokenizer, input_text, device):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Find the position of the last EOS token (which comes before the answer)\n",
    "    sep_positions = get_sep_position(inputs.input_ids, tokenizer.eos_token_id, skip=1)\n",
    "    \n",
    "    # Create labels: -100 for all tokens except the answer\n",
    "    labels = torch.full_like(inputs.input_ids, -100)\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        labels[i, sep_positions[i]+1:] = inputs.input_ids[i, sep_positions[i]+1:]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.compute_loss(inputs['input_ids'], labels=labels)\n",
    "    \n",
    "    return outputs.loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = 'math_qa'\n",
    "# data = 'aqua_rat'\n",
    "# data = 'trivia_qa'\n",
    "# data='commonsenseqa'\n",
    "# data = 'gsm8k'\n",
    "data='strategy-qa'\n",
    "\n",
    "def analyze_cot_removal(model, tokenizer, dataset, max_sentences_to_remove, num_samples_do, device):\n",
    "    results = []\n",
    "    \n",
    "    for t in range(1, max_sentences_to_remove + 1):\n",
    "        loss_differences = []\n",
    "        example_results = []\n",
    "\n",
    "        cnt = 0\n",
    "        for example in tqdm(dataset, desc=f\"Analyzing {t} sentence(s) removal\"):\n",
    "            src, cot, ans = example['src'], example['cot'], example['ans']\n",
    "            sentences = split_cot_sentences(cot)\n",
    "            \n",
    "\n",
    "            if len(sentences) < t:\n",
    "                continue\n",
    "            \n",
    "            original_input = format_input(src, cot, ans, tokenizer.eos_token)\n",
    "            original_loss = compute_answer_loss(model, tokenizer, original_input, device)\n",
    "            # torch.cuda.empty_cache()\n",
    "            \n",
    "            # Remove t random sentences\n",
    "            removed_sentences = random.sample(sentences, t)\n",
    "            \n",
    "            remaining_sentences = [s for s in sentences if s not in removed_sentences]\n",
    "            if len(remaining_sentences) <1:\n",
    "                continue\n",
    "\n",
    "            modified_cot = \" \".join(remaining_sentences).strip()\n",
    "\n",
    "            modified_input = format_input(src, modified_cot, ans, tokenizer.eos_token)\n",
    "            modified_loss = compute_answer_loss(model, tokenizer, modified_input, device)\n",
    "            # torch.cuda.empty_cache()\n",
    "            \n",
    "            loss_difference =  original_loss - modified_loss\n",
    "            loss_differences.append(loss_difference)\n",
    "            \n",
    "            example_results.append({\n",
    "                \"original_text\": original_input,\n",
    "                \"removed_cot_text\": modified_input,\n",
    "                \"removed_sentences\": \" \".join(removed_sentences).strip(),\n",
    "                'original_loss': original_loss,\n",
    "                'modified_loss': modified_loss,\n",
    "                \"loss_difference\": loss_difference\n",
    "            })\n",
    "            cnt +=1\n",
    "            if cnt == num_samples_do:\n",
    "                break\n",
    "        \n",
    "        avg_loss_difference = sum(loss_differences) / len(loss_differences) if loss_differences else 0\n",
    "        results.append({\n",
    "            \"sentences_removed\": t,\n",
    "            \"avg_loss_difference\": avg_loss_difference,\n",
    "            \"num_samples\": len(loss_differences)\n",
    "        })\n",
    "        \n",
    "        # Save detailed results to a JSON file\n",
    "        os.makedirs(f\"motivation-new/{data}\", exist_ok=True)\n",
    "        with open(f'motivation-new/{data}/cot_removal_results_{t}.jsonl', 'w') as f:\n",
    "            for item in example_results:\n",
    "                json.dump(item, f)\n",
    "                f.write('\\n')\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# def analyze_cot_removal(model, tokenizer, dataset, max_sentences_to_remove, num_samples_do, device):\n",
    "#     results = []\n",
    "    \n",
    "#     for t in range(1, max_sentences_to_remove + 1):\n",
    "#         loss_differences = []\n",
    "#         example_results = []\n",
    "\n",
    "#         cnt = 0\n",
    "#         for example in tqdm(dataset, desc=f\"Analyzing {t} sentence(s) removal\"):\n",
    "#             src, cot, ans = example['src'], example['cot'], example['ans']\n",
    "#             sentences = split_cot_sentences(cot)\n",
    "            \n",
    "#             if len(sentences) < t:\n",
    "#                 continue\n",
    "            \n",
    "#             original_input = format_input(src, cot, ans, tokenizer.eos_token)\n",
    "#             original_loss = compute_answer_loss(model, tokenizer, original_input, device)\n",
    "#             # torch.cuda.empty_cache()\n",
    "            \n",
    "#             # Create a probability distribution that gives higher weight to earlier sentences\n",
    "#             probabilities = np.linspace(1.0, 0.1, num=len(sentences))\n",
    "#             probabilities /= probabilities.sum()  # Normalize to make it a valid probability distribution\n",
    "            \n",
    "#             # Remove t sentences with weighted probabilities\n",
    "#             removed_indices = np.random.choice(len(sentences), size=t, replace=False, p=probabilities)\n",
    "#             removed_sentences = [sentences[i] for i in removed_indices]\n",
    "            \n",
    "#             remaining_sentences = [s for i, s in enumerate(sentences) if i not in removed_indices]\n",
    "#             if len(remaining_sentences) < 1:\n",
    "#                 continue\n",
    "\n",
    "#             modified_cot = \" \".join(remaining_sentences).strip()\n",
    "\n",
    "#             modified_input = format_input(src, modified_cot, ans, tokenizer.eos_token)\n",
    "#             modified_loss = compute_answer_loss(model, tokenizer, modified_input, device)\n",
    "#             # torch.cuda.empty_cache()\n",
    "            \n",
    "#             loss_difference = original_loss - modified_loss\n",
    "#             loss_differences.append(loss_difference)\n",
    "            \n",
    "#             example_results.append({\n",
    "#                 \"original_text\": original_input,\n",
    "#                 \"removed_cot_text\": modified_input,\n",
    "#                 \"removed_sentences\": \" \".join(removed_sentences).strip(),\n",
    "#                 'original_loss': original_loss,\n",
    "#                 'modified_loss': modified_loss,\n",
    "#                 \"loss_difference\": loss_difference\n",
    "#             })\n",
    "#             cnt +=1\n",
    "#             if cnt == num_samples_do:\n",
    "#                 break\n",
    "        \n",
    "#         avg_loss_difference = sum(loss_differences) / len(loss_differences) if loss_differences else 0\n",
    "#         results.append({\n",
    "#             \"sentences_removed\": t,\n",
    "#             \"avg_loss_difference\": avg_loss_difference,\n",
    "#             \"num_samples\": len(loss_differences)\n",
    "#         })\n",
    "        \n",
    "#         # Save detailed results to a JSON file\n",
    "#         os.makedirs(f\"motivation-new2/{data}\", exist_ok=True)\n",
    "#         with open(f'motivation-new2/{data}/cot_removal_results_{t}.jsonl', 'w') as f:\n",
    "#             for item in example_results:\n",
    "#                 json.dump(item, f)\n",
    "#                 f.write('\\n')\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.38s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"  # Update this to your model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = Mistral.from_pretrained(model_name)\n",
    "\n",
    "config = MistralConfig(model_name)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Mistral(config).to(device, dtype=torch.bfloat16)\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing 1 sentence(s) removal:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing 1 sentence(s) removal:  11%|█         | 111/1000 [00:04<00:36, 24.25it/s]\n",
      "Analyzing 2 sentence(s) removal:  19%|█▉        | 188/1000 [00:05<00:25, 31.95it/s]\n",
      "Analyzing 3 sentence(s) removal:  60%|██████    | 601/1000 [00:08<00:05, 67.67it/s]\n",
      "Analyzing 4 sentence(s) removal: 100%|██████████| 1000/1000 [00:05<00:00, 199.28it/s]\n",
      "Analyzing 5 sentence(s) removal: 100%|██████████| 1000/1000 [00:01<00:00, 668.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Sentences removed: 1\n",
      "Average loss difference: -0.1220\n",
      "Number of samples: 100\n",
      "Sentences removed: 2\n",
      "Average loss difference: -0.1394\n",
      "Number of samples: 100\n",
      "Sentences removed: 3\n",
      "Average loss difference: -0.2038\n",
      "Number of samples: 100\n",
      "Sentences removed: 4\n",
      "Average loss difference: -0.2156\n",
      "Number of samples: 47\n",
      "Sentences removed: 5\n",
      "Average loss difference: -0.3876\n",
      "Number of samples: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = f\"data/{data}/{data}_train.txt\"  # Update this to your dataset path\n",
    "num_samples = 1000  # Number of samples to analyze\n",
    "num_samples_do = 100\n",
    "max_length = 512  # Max sequence length\n",
    "max_sentences_to_remove = 5  # Maximum number of sentences to remove\n",
    "\n",
    "dataset = CoTAnalysisDataset(tokenizer, file_path, max_length, num_samples)\n",
    "\n",
    "results = analyze_cot_removal(model, tokenizer, dataset, max_sentences_to_remove, num_samples_do, device)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nResults:\")\n",
    "for result in results:\n",
    "    print(f\"Sentences removed: {result['sentences_removed']}\")\n",
    "    print(f\"Average loss difference: {result['avg_loss_difference']:.4f}\")\n",
    "    print(f\"Number of samples: {result['num_samples']}\")\n",
    "    # print(\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
